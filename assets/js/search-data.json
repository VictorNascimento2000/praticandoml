{
  
    
        "post0": {
            "title": "Title",
            "content": "Muitas pessoas que estudam análise dados sabem aplicar o famoso PCA para reduzir a dimensionalidade de uma matriz, mas mais importante do que escrever as poucas linhas da função que faz isso em Python ou R é entender o que está acontecendo por dentro do método, ou seja, porque e como ele funciona. . Suponha que temos a seguinte matriz que eu mesmo criei, onde as colunas são atributos e as linhas são observações. Note que ela é bem simples, pois queremos preservar o máximo da intuição geométrica possível. . import pandas as pd data = {&#39;Atributo_1&#39;:[1, 3, 3, 4, 5, 6, 6, 8], &#39;Atributo_2&#39;:[1, 1, 1, 1.7, 2, 1, 1.5, 2], &#39;Atributo_3&#39;:[10, 20, 30, 10, 30, 40, 30, 50]} df = pd.DataFrame(data) df . Atributo_1 Atributo_2 Atributo_3 . 0 1 | 1.0 | 10 | . 1 3 | 1.0 | 20 | . 2 3 | 1.0 | 30 | . 3 4 | 1.7 | 10 | . 4 5 | 2.0 | 30 | . 5 6 | 1.0 | 40 | . 6 6 | 1.5 | 30 | . 7 8 | 2.0 | 50 | . import plotly.express as px fig = px.scatter_3d(df, x=&#39;Atributo_1&#39;, y=&#39;Atributo_2&#39;, z=&#39;Atributo_3&#39;) fig.show() . A pergunta que queremos responder é: Em vez de usarmos três dimensões como temos na figura acima, poderíamos usar apenas duas sem prejudicar a qualidade dos dados? Vamos pensar em como podemos fazer isso. Para termos uma nova figura, mas com duas dimensões, precisamos criar um plano. Porém, queremos um plano muito específico, um que tente preservar todas as informações que tinhamos anteriormente. Então como encontrar os dois vetores que serão a base desse plano? Vamos tentar criar alguma intuição sobre isso. . import seaborn as sns sns.pairplot(df) . &lt;seaborn.axisgrid.PairGrid at 0x128fc064b48&gt; . Plotando os eixos da nossa figura inicial, mas agora dois a dois, fingindo que não há um terceiro eixo, vemos que alguns deles parecem estar correlacionados como é o caso da figura no canto superior direito e inferior esquerdo. Isso significa que existe uma reta que pode capturar boa parte da variação dos dados desse gráfico. Ou seja, teremos uma reta que contém uma grande parcela da informação dos dois eixos, ela é o que chamamos de Componente Principal e será um dos vetores da base do nosso plano. . Após termos nosso plano, precisamos saber como representar os pontos que estavam no R³ no R². Para isso fazemos a projeção ortogonal dos pontos no plano. Isso pode ser mais facilmente compreendido de forma visual. Suponha que esse seja o melhor plano que captura a variação dos nossos dados. Nosso próximo passo é projetar nele os pontos que está no R³. . import numpy as np import plotly.graph_objs as go fig = px.scatter_3d(df, x=&#39;Atributo_1&#39;, y=&#39;Atributo_2&#39;, z=&#39;Atributo_3&#39;) x = np.linspace(1,8,8) y = np.linspace(1,5,2) X,Y = np.meshgrid(x,y) Z = (2*X + 3*Y + 15) fig.add_trace(go.Surface(x=x, y=y, z=Z, showscale=False, colorscale=&#39;Blues&#39;)) fig.show() . Isso pode ser visto na imagem abaixo também, onde o ponto x seria uma das nossas bolinhas azuis. . . Tendo uma visão geral agora vamos discutir como realmente calcular os vetores da base desse plano e sua projeção. Em primeiro lugar veja que nós devemos realizar uma etapa de pré-processamento para que os dados sejam igualmente tratados. Caso contrário, atributos com valores absoutos muito grandes podem receber maior importância, o que não deveria acontecer. Portanto a primeira etapa consiste em subtrairmos de cada atributo sua média. Com os vetores resultados dividimos cada um pelo seu desvio-padrão. . x = df.values Media = np.mean(x, axis=0) matriz_media = [Media,]*x.shape[0] M = x - matriz_media desvio_padrao = np.std(M, axis = 0) scaled_x = (M/desvio_padrao) scaled_x . array([[-1.69774938, -0.93632918, -1.34715063], [-0.72760688, -0.93632918, -0.57735027], [-0.72760688, -0.93632918, 0.19245009], [-0.24253563, 0.70224688, -1.34715063], [ 0.24253563, 1.40449377, 0.19245009], [ 0.72760688, -0.93632918, 0.96225045], [ 0.72760688, 0.23408229, 0.19245009], [ 1.69774938, 1.40449377, 1.73205081]]) . scaled_data = {&#39;Atributo_1&#39;:scaled_x[:,0], &#39;Atributo_2&#39;:scaled_x[:,1], &#39;Atributo_3&#39;:scaled_x[:,2]} scaled_df = pd.DataFrame(scaled_data) fig = px.scatter_3d(scaled_df, x=&#39;Atributo_1&#39;, y=&#39;Atributo_2&#39;, z=&#39;Atributo_3&#39;) fig.show() . Agora temos duas grande diferenças em relação ao gráfico original. Primeiro, os dados estão centrados na origem, pois agora a média deles é zero. Segundo, todos os eixos tem a mesma importância. Nenhum neles tem números absurdamente maiores que dos outros. Apesar da diferença note como a estrutura dos dados é exatamente a mesma. Só os valores do eixo estão diferentes. . . Agora vamos pensar em como descobrir qual o melhor plano para projetarmos nossos dados. A equação que mostra onde queremos chegar pode ser dada por $Y = PX$. Vamos discutir cada uma dessas matrizes. $P$ é uma matriz de projeção, ou seja ela projeta algo em algum lugar. Ela vai projetar pontos da matriz $X$ em um novo espaço vetorial, que no nosso caso é um plano. De outro modo podemos dizer que ela é uma transformação linear $T$ onde $T: R³ rightarrow R²$. $Y$ por sua vez é a matriz resultante com as novas coordenadas. Um detalhe é que ao fazermos a multiplicação $PX$ estamos projetando as linhas de $X$ no novo espaço vetorial. Portanto, precisamos transpor nossa matriz original para que suas linhas sejam os atributos, pois são eles quem nos queremos projetar. Dessa forma temos que a primeira coluna de $Y$ é $Px_{1}$ onde $x_{1}$ é a primeira coluna de $X$ e assim por diante. . . Nosso objetivo agora é descobrir quem é $P$, mas vamos analisar $Y$ para descobrir alguma coisa a respeito. Queremos que $Y$ não tenha informação redundante, ou seja, que ele não tenho variáveis correlacionadas, o que significa que buscamos um covariância igual 0 entre as variáveis de $Y$. Por outro lado queremos maximizar a variância, pois isso significa que estaremos conseguindo captar a maior quantidade possível de informações. Portanto nossa matriz de convariância de $Y$ deve ser uma matriz diagonal. Sendo $C_{y}$ a covariância de $Y$ então temos que: . $C_{y} = frac{1}{n-1}YY^T = frac{1}{n-1}(PX)(PX)^T = frac{1}{n-1}PXX^TP^T = frac{1}{n-1}PSP^T$ . Onde $S=XX^T$ e $n$ é o número de observações da matriz $Y$. Veja que $S$ é uma matriz simétrica, portanto ela admite uma decomposição espectral tal que $S = Q Lambda Q^T$. Portanto temos que $C_{y} = frac{1}{n-1}PQ Lambda Q^TP^T = frac{1}{n-1}(PQ) Lambda (PQ)^T $. Como dito anteriormente, queremos que $C_{y}$ seja uma matriz diagonal então $PQ$ tem que ser a matriz identidade. Para isso acontecer pode simplesmente tomar $P=Q^T$, dessa forma finalmente chegamos em $C_{y} = frac{1}{n-1} Lambda$, uma matriz diagonal onde $ Lambda$ contém a importância de cada componente. . Por fim, perceba que podemos aplicar SVD em $X$ então $X=U Sigma V^T$, logo $S=XX^T = U Sigma V^T V Sigma^T U^T = U Sigma Sigma^T U^T$ onde podemos perceber que $U=E$, portanto $P=U$. . Vamos fazer enfim terminar a matriz do início essa discussão. Lembrando que ela já foi normalizada e processada para ter média igual a zero, precisamos agora transpô-la para que seus atributos estejam nas linhas e então seguir com o que foi explicado. . scaled_x_t = scaled_x.T S = scaled_x_t @ scaled_x_t.T u, s, vh = np.linalg.svd(S) Y = u.T @ scaled_x_t Y.T . array([[ 2.34592819, -0.17881354, 0.19628223], [ 1.26617707, 0.35971583, -0.08281041], [ 0.81998399, 0.79804911, 0.36593033], [ 0.59695461, -1.36484191, -0.38424052], [-0.95448338, -1.01078694, 0.36842169], [-0.57654615, 1.38667652, -0.27707901], [-0.70080773, -0.00617283, -0.36065738], [-2.79720661, 0.01617376, 0.17415309]]) . Para confirmar os resultados podemos usar as funções prontas. Nosso resultado está correto apesar do sinal diferente, pois as componentes são vetores que apontam direções. Mudar o sinal não muda a variância contida no vetor (para mais informações: https://stats.stackexchange.com/questions/88880/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers). . from sklearn.decomposition import PCA pca = PCA(n_components=3) pca_decomp = pca.fit(scaled_x) pca_x = pca_decomp.transform(scaled_x) pca_x . array([[-2.34592819, -0.17881354, -0.19628223], [-1.26617707, 0.35971583, 0.08281041], [-0.81998399, 0.79804911, -0.36593033], [-0.59695461, -1.36484191, 0.38424052], [ 0.95448338, -1.01078694, -0.36842169], [ 0.57654615, 1.38667652, 0.27707901], [ 0.70080773, -0.00617283, 0.36065738], [ 2.79720661, 0.01617376, -0.17415309]]) . Podemos também calcular o quão importante cada componenete é. Isso é bem simples, basta dividir a variância da componente pela variância total. . variancia = np.var(Y, axis = 1) sum_var = np.sum(variancia) print(&quot;Variância explicada pela primeira componente: &quot;, variancia[0]/desvio_padrao_quadrado) print(&quot;Variância explicada pela segunda componente: &quot;, variancia[1]/desvio_padrao_quadrado) print(&quot;Variância explicada pela terceira componente: &quot;, variancia[2]/desvio_padrao_quadrado) . Variância explicada pela primeira componente: 0.7372605113484414 Variância explicada pela segunda componente: 0.23357947093949818 Variância explicada pela terceira componente: 0.029160017712060476 . Apenas confirmando: . pca.explained_variance_ratio_ . array([0.73726051, 0.23357947, 0.02916002]) . Por último, vamos mostrar no gráfico o plano gerado pelo PCA. Sabemos que as linhas de $U$ são os vetores que capturam a maior variância dos dados, portanto como queremos um plano pegaremos as duas primerias linhas como base. . u . array([[-0.65305667, 0.10327976, -0.75023348], [-0.48739668, -0.81553889, 0.31199487], [-0.57962182, 0.56941164, 0.58293132]]) .",
            "url": "https://victornascimento2000.github.io/praticandoml/2020/11/16/PCA.html",
            "relUrl": "/2020/11/16/PCA.html",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Usando SVD para comprimir imagens",
            "content": "Uma aplicação muito interessante da Decomposição em Valores Singulares, mais conhecida pela sua sigla SVD (Singular Values Decomposition) é a de conseguir criar uma aproximação da matrix original, mas usando menos informações. Isso fica mais claro na prática. Suponha que temos a seguinte matrix: . . É claro que as três primeiras colunas são linearmente dependentes e as outras não. Portanto temos uma matriz de posto 3, ou seja, ela tem três valores singulares diferentes de zero. Aplicando SVD podemos tentar nos aproximar da matriz original usando os dois primeiros valores singulares. Primeiro vamos ver como a matriz fica usando somente um. . . É possível ver algumas semelhanças à matriz original, mas vale a pena usar mais um valor singular. . . Note como a matriz resultante é muito parecida com a matriz original. Se usassemos o último valor singular, teríamos a própria matriz A, mas nosso objetivo é conseguir aproximações! Vamos ver agora como essa mesma ideia pode ser usada para comprimir imagens. . import matplotlib.image as img from skimage.io import imread, imshow import matplotlib.pyplot as plt import numpy as np from math import log url = &quot;https://i.imgur.com/QuM1yXv.jpeg&quot; image = imread(url, as_gray=True) plt.imshow(image, cmap=plt.get_cmap(&#39;gray&#39;)) plt.show() . . Temos aqui uma imagem com 2160 pixels de largura e 3840 pixels de comprimento. Cada pixel é um número. Essa imagem pode ser expressa por uma matriz onde cada número é a cor de um pixel. Como é uma imagem preto e branco, temos uma matriz com 2160 linhas e 3840 colunas, ou seja, ela tem 8.294.400 bytes! . [[0.12413059 0.13197373 0.12805216 ... 0.09780275 0.10564588 0.10956745] [0.12020902 0.12805216 0.12805216 ... 0.10564588 0.10956745 0.11348902] [0.11628745 0.12413059 0.12805216 ... 0.10564588 0.10956745 0.10956745] ... [0.10564588 0.12133216 0.12917529 ... 0.05072824 0.05072824 0.06249294] [0.11741059 0.12133216 0.12525373 ... 0.05072824 0.0546498 0.06641451] [0.12133216 0.12133216 0.12525373 ... 0.05857137 0.06249294 0.07033608]] . A questão é: realmente precisamos de todos esses números? Usando a decomposição singular fica claro que a resposta é não. Podemos encontrar uma ótima aproximação para essa matriz. Primeiro vamos ver como seus valores singulares se comportam. . #gráfico dos valores singulares u, s, vt = np.linalg.svd(image) plt.plot(np.arange(2160),s) plt.ylabel(r&#39;valor de $ sigma$&#39;, fontsize=13) plt.show() . . Fica evidente o quão rápido decrescem, portanto vamos colocar em uma escala logarítimica. . s_log = [log(y,10) for y in s] plt.plot(np.arange(2160),s_log) plt.ylabel(r&#39;valor de log($ sigma$)&#39;, fontsize=13) plt.show() . Após os 50 ou 100 primeiros valores singulares é notável como eles decrescem rapidamente. Dessa forma surge a questão: Quantos valores singulares queremos para que tenhamos uma boa aproximação da matriz original? Para isso usamos a regrão do dedão que consiste em manter pelo menos 80 ou 90% da soma dos quadrados dos valores singulares originais. Isso pode ser dado pela seguinte equação: . . Podemos então fazer um função para que recebe os valores singulares como parâmetros e retorna a posição do último que obedece a regra do dedão. . def escolheK(s): &quot;&quot;&quot; Retorna o menor k tal que a soma dos quadrados dos k primeiros elementos do array s é &gt;= 90% da soma dos quadrados dos elementos de s. Entrada: s é um vetor contendo os valores singulares em ordem decrescente &quot;&quot;&quot; k = 0 t = 0 m, n = s.reshape((-1,1)).shape total = s.reshape((-1,1)).T @ s.reshape((-1,1)) for k in range(m): t = s[k]**2 + t if t/total &gt; 0.90: k += 1 # somando um pois começamos a contar o primeiro valor singular como zero break return k . print(escolheK(s)) . 7 . Vamos reconstruir a imagem usando os 7 maiores valores singulares. . m, n = image.shape img_rank_7 = np.zeros((m, n)) for i in range(7): img_rank_7 = s[i]*u[:,i].reshape((-1,1)) @ vt[i,:].reshape((1,-1)) + img_rank_7 plt.imshow(img_rank_7, cmap=plt.get_cmap(&#39;gray&#39;)) plt.show() . Porém a imagem ainda não está boa. Vamos usar 15 valores singulares. . Dessa forma a imagem se torna entendível, mas vamos usar mais alguns valores singulares para deixá-la mais bonita. . img_rank_30 = np.zeros((m, n)) for i in range(30): img_rank_30 = s[i]*u[:,i].reshape((-1,1)) @ vt[i,:].reshape((1,-1)) + img_rank_30 plt.imshow(img_rank_30, cmap=plt.get_cmap(&#39;gray&#39;)) plt.show() . Dessa forma reconstruimos a imagem original de maneira razoavelmente boa. Mas em vez de usarmos 8.294.400 bytes agora usamos 30*2160 + 30 + 30*3840 = 180.030 bytes. Isso significa que temos um arquivo 46 vezes menor! .",
            "url": "https://victornascimento2000.github.io/praticandoml/jupyter/2020/11/14/SVD.html",
            "relUrl": "/jupyter/2020/11/14/SVD.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://victornascimento2000.github.io/praticandoml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://victornascimento2000.github.io/praticandoml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://victornascimento2000.github.io/praticandoml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://victornascimento2000.github.io/praticandoml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}